{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Skillfactory---Практический-Machine-Learning\" data-toc-modified-id=\"Skillfactory---Практический-Machine-Learning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Skillfactory - Практический Machine Learning</a></div><div class=\"lev2 toc-item\"><a href=\"#15/02/2018---Методы-регрессии\" data-toc-modified-id=\"15/02/2018---Методы-регрессии-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>15/02/2018 - Методы регрессии</a></div><div class=\"lev2 toc-item\"><a href=\"#Функции-потерь\" data-toc-modified-id=\"Функции-потерь-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Функции потерь</a></div><div class=\"lev1 toc-item\"><a href=\"#Алгоритмы\" data-toc-modified-id=\"Алгоритмы-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Алгоритмы</a></div><div class=\"lev2 toc-item\"><a href=\"#Метод-ближайшего-соседа\" data-toc-modified-id=\"Метод-ближайшего-соседа-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Метод ближайшего соседа</a></div><div class=\"lev2 toc-item\"><a href=\"#Деревья-решений\" data-toc-modified-id=\"Деревья-решений-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Деревья решений</a></div><div class=\"lev2 toc-item\"><a href=\"#Линейная-регрессия\" data-toc-modified-id=\"Линейная-регрессия-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Линейная регрессия</a></div><div class=\"lev3 toc-item\"><a href=\"#Normal-Equation\" data-toc-modified-id=\"Normal-Equation-231\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Normal Equation</a></div><div class=\"lev4 toc-item\"><a href=\"#Задачка\" data-toc-modified-id=\"Задачка-2311\"><span class=\"toc-item-num\">2.3.1.1&nbsp;&nbsp;</span>Задачка</a></div><div class=\"lev3 toc-item\"><a href=\"#Градиентный-спуск\" data-toc-modified-id=\"Градиентный-спуск-232\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Градиентный спуск</a></div><div class=\"lev3 toc-item\"><a href=\"#Природа-зависимости\" data-toc-modified-id=\"Природа-зависимости-233\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Природа зависимости</a></div><div class=\"lev4 toc-item\"><a href=\"#Пример\" data-toc-modified-id=\"Пример-2331\"><span class=\"toc-item-num\">2.3.3.1&nbsp;&nbsp;</span>Пример</a></div><div class=\"lev2 toc-item\"><a href=\"#Ладно,-давайте-дальше-в-sklearn-=)\" data-toc-modified-id=\"Ладно,-давайте-дальше-в-sklearn-=)-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Ладно, давайте дальше в sklearn =)</a></div><div class=\"lev3 toc-item\"><a href=\"#Переобучениенедообучение,-мультиколлинеарность-и-регуляризация\" data-toc-modified-id=\"Переобучениенедообучение,-мультиколлинеарность-и-регуляризация-241\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Переобучение\\недообучение, мультиколлинеарность и регуляризация</a></div><div class=\"lev2 toc-item\"><a href=\"#Выбросы-в-данных\" data-toc-modified-id=\"Выбросы-в-данных-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Выбросы в данных</a></div><div class=\"lev3 toc-item\"><a href=\"#RANSAC-регрессия\" data-toc-modified-id=\"RANSAC-регрессия-251\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>RANSAC регрессия</a></div><div class=\"lev3 toc-item\"><a href=\"#Robust-Estimators\" data-toc-modified-id=\"Robust-Estimators-252\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Robust Estimators</a></div><div class=\"lev3 toc-item\"><a href=\"#Добавим-остальные-признаки-в-простую-модель-модель\" data-toc-modified-id=\"Добавим-остальные-признаки-в-простую-модель-модель-253\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Добавим остальные признаки в простую модель модель</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skillfactory - Практический Machine Learning\n",
    "## 15/02/2018 - Методы регрессии\n",
    "\n",
    "<center> Шестаков Андрей </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче классификации зависимой переменной $y$ была \"метка класса\" - по сути категориальная переменная (отдаст кредит или нет, выживет или нет, тип цветка). <br\\>\n",
    "В задаче регрессии зависимая переменная $y \\in \\mathbb{R}^n$ (стоимость квартиры, количество кликов на баннер, объемы покупок). <br\\>\n",
    "\n",
    "Схемы алгоритмов, по большей части, остаются прежними (с точностью до функции потерь)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from ipywidgets import interact, IntSlider\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $y^{(i)}$ - значение целевой переменной для $i$-го объекта, а $\\hat{y}^{(i)}=a(x^{(i)})$ - её оценка алгоритмом $a(x)$.Чаще всего качество регрессионных методов оценивается по следующим функциям потерь:\n",
    "\n",
    "**1. (R)MSE ((Root) Mean Squared Error) - (Корень из) Среднеквадратичное отклонение**\n",
    "\n",
    "$$ L(a, y) = \\frac{1}{N}\\sum\\limits_i^N (y^{(i)} - \\hat{y}^{(i)})^2$$\n",
    "\n",
    "**2. MAE (Mean Absolute Error) - Среднее абсолютное отклонение**\n",
    "\n",
    "$$ L(a, y) = \\frac{1}{N}\\sum\\limits_i |y^{(i)} - \\hat{y}^{(i)}|$$\n",
    "\n",
    "**3. RSE (Relative Squared Error) - Относительное квадратичное отклонение**\n",
    "\n",
    "$$ L(a, y) = \\sqrt\\frac{\\sum\\limits_i (y^{(i)} - \\hat{y}^{(i)})^2}{\\sum\\limits_i (y^{(i)} - \\bar{y})^2}$$\n",
    "\n",
    "**4. RAE (Relative Absolute Error) - Относительное абсолютное отклонение (?)**\n",
    "\n",
    "$$ L(a, y) = \\frac{\\sum\\limits_i |y^{(i)} - \\hat{y}^{(i)}|}{\\sum\\limits_i |y^{(i)} - \\bar{y}|}$$\n",
    "\n",
    "**5. RMSLE (Root Mean Squared Logarithmic Error) - (?!)**\n",
    "\n",
    "$$ L(a, y) = \\sqrt{\\frac{1}{N}\\sum\\limits_i^N(\\log(y^{(i)} + 1) - \\log(\\hat{y}^{(i)} + 1))^2}$$\n",
    "\n",
    "**6. MAPE (Mean Absolute Persentage Error) - Среднее абсолютное отклонение в процентах **\n",
    "\n",
    "$$ L(a, y) = \\frac{100}{N} \\sum\\limits_i\\left|\\frac{ y^{(i)} - \\hat{y}^{(i)}}{y^{(i)}}\\right|$$\n",
    "\n",
    "\n",
    "**7. Под предметную область **\n",
    "\n",
    "* Стоимость заморозки средств, стоимость хранения товара и тп\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Вопросы **\n",
    "* Как интерпретируются ошибки (3) и (4)?\n",
    "* В чем особенность ошибки (5) и (6)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 100\n",
    "y_hat = np.linspace(0, 300, 151)\n",
    "# log error\n",
    "error1 = np.sqrt((np.log(y+1) - np.log(y_hat + 1))**2)\n",
    "\n",
    "# squared error\n",
    "error2 = (y - y_hat)**2 /1000.\n",
    "\n",
    "plt.plot(y_hat, error1, label='RMSLE')\n",
    "plt.plot(y_hat, error2, label='MSE')\n",
    "plt.xlabel('$\\hat{y}$')\n",
    "plt.ylabel('Error')\n",
    "plt.title('true value y = %.1f' % y)\n",
    "plt.legend()\n",
    "plt.ylim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Недопрогнозировать дороже, чем перепрогнозировать\n",
    "* Учет эффекта масштаба"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод ближайшего соседа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод минимально отличается от варианта с классификацией. <br\\> По прежнему считаем меру \"близости\" между объектами, а затем усредняем значения целевого признака у *k* ближайших соседей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим данные по характеристикам автомобилей Honda Accord. Названия столбцов говорят сами за себя ([обучение](http://bit.ly/1gIQs6C), [тест](http://bit.ly/IYPHrK)). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('http://bit.ly/1gIQs6C')\n",
    "df_test = pd.read_csv('http://bit.ly/IYPHrK')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.plot(y='price', x='mileage', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотроим регрессор на k ближайших соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.mileage.values.reshape(-1, 1)\n",
    "y_train = df_train.price.values\n",
    "\n",
    "X_test = df_test.mileage.values.reshape(-1, 1)\n",
    "y_test = df_test.price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=10, weights='uniform', metric='manhattan')\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = knn.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='blue')\n",
    "plt.xlabel('mileage')\n",
    "plt.ylabel('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_knn(k=5):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, weights='uniform', metric='manhattan')\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    X_train_s = np.sort(X_train, axis=0)\n",
    "    y_hat = knn.predict(X_train_s.reshape(-1,1))\n",
    "    plt.xlabel('mileage')\n",
    "    plt.ylabel('price')\n",
    "\n",
    "    \n",
    "    plt.scatter(X_train, y_train, c='r', label='actual data')\n",
    "    plt.plot(X_train_s, y_hat, c='b', label='knn, $k=%d$' % k)\n",
    "    plt.legend(loc=2)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = interact(plot_knn, k=IntSlider(min=1, max=10, value=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_report(X_s, y_s, model, labels, score=mean_absolute_error):\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, len(labels)))\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        X = X_s[idx]\n",
    "        y_true = y_s[idx]\n",
    "\n",
    "        y_hat = model.predict(X)\n",
    "\n",
    "        # Scatter\n",
    "        ax[0].scatter(y_hat, y_true, color = colors[idx], alpha=0.3,\n",
    "                      label='%s Score - %2.4f' % (label, score(y_true, y_hat)))\n",
    "\n",
    "        # Resid\n",
    "        resid = y_true - y_hat\n",
    "        ax[1].scatter(resid, y_true, color = colors[idx], alpha=0.3, label=label)\n",
    "\n",
    "        # Distr\n",
    "        ax[2].hist(y_hat, alpha=0.5, label=label, color = colors[idx], normed=True)\n",
    "\n",
    "        # Resid\n",
    "        resid = y_true - y_hat\n",
    "        ax[3].scatter(resid, y_hat, color = colors[idx], alpha=0.3, label=label)\n",
    "\n",
    "    ax[0].legend(loc=4)\n",
    "    ax[0].set_xlabel('$\\hat{y}$')\n",
    "    ax[0].set_ylabel('$y$')\n",
    "\n",
    "    ax[1].legend(loc=2)\n",
    "    ax[1].set_xlabel('$resid$')\n",
    "    ax[1].set_ylabel('$y$')\n",
    "\n",
    "    ax[2].legend(loc=2)\n",
    "    ax[2].set_xlabel('$\\hat{y}$')\n",
    "\n",
    "    ax[3].legend(loc=2)\n",
    "    ax[3].set_xlabel('$resid$')\n",
    "    ax[3].set_ylabel('$\\hat{y}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_report([X_train, X_test], [y_train, y_test], knn, \n",
    "                  ['train', 'test'], score=mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Деревья решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним себе на основе чего строится дерево решений. В каждом новом узле выбирается признак и его значение, максимизирущее прирост информативности:\n",
    "$$ Gain(S, A) = I(S) - \\sum\\limits_v\\frac{|S_v|}{|S|}\\cdot I(S_v),$$ где $A$ - это некий атрибут, а $v$ - его значения, I(A) - одна из мер неопределенности, например:\n",
    "* Gini index $I(S) = 1 - \\sum\\limits_k (p_k)^2$\n",
    "* Entropy  $I(S) = -\\sum\\limits_k p_k \\log(p_k)$\n",
    "* Missclassification error  $I(S) = 1 - \\max\\limits_k p_k$\n",
    "\n",
    "Как видно, для того, чтобы посчитать $I(S)$ нам нужно знать пропорции классов $p_k$. Но у нас же задача регрессии!\n",
    "\n",
    "В этом случае, можно считать мерой неопределенности разброс значений целевого признака `y` (среднюю квадратичную ошибку относительно среднего):\n",
    "$$I(S) = \\frac{1}{|S|} \\sum\\limits_{i \\in S} (y_i - c)^2 $$ \n",
    "$$ c = \\frac{1}{|S|}\\sum\\limits_{i \\in S} y_i $$\n",
    "\n",
    "или среднюю абсолютную ошибку относительно медианы\n",
    "\n",
    "$$I(S) = \\frac{1}{|S|} \\sum\\limits_{i \\in S} |y_i - c| $$ \n",
    "$$ c = median(\\{y_i\\}) \\ i \\in S$$\n",
    "\n",
    "\n",
    "Таким образом, дерево будет выбирать такие значение признаков из `X`, что разброс `y` будет наименьшим.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = tree.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='blue')\n",
    "plt.xlabel('mileage')\n",
    "plt.ylabel('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dec_reg(depth=1, criterion='mse', ):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    fig.set_figwidth(20)\n",
    "    \n",
    "    X_train_s = np.sort(X_train, axis=0)\n",
    "    \n",
    "    tree = DecisionTreeRegressor(criterion=criterion, max_depth=depth)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_hat = tree.predict(X_train_s.reshape(-1,1))\n",
    "    ax[1].set_xlabel('mileage')\n",
    "    ax[1].set_ylabel('price')\n",
    "    ax[1].scatter(X_train, y_train, label='actual data')\n",
    "    ax[1].plot(X_train_s, y_hat, c='blue', label='decision tree \\nregression')\n",
    "    ax[1].legend(loc=2)\n",
    "    \n",
    "    try:\n",
    "        with open('tree.dot', 'w') as fout:\n",
    "            export_graphviz(tree, out_file=fout, feature_names=['mileage'])\n",
    "        command = [\"dot\", \"-Tpng\", \"tree.dot\", \"-o\", \"tree.png\"]\n",
    "        subprocess.check_call(command)\n",
    "        ax[0].imshow(plt.imread('tree.png'))\n",
    "        ax[0].axis(\"off\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = interact(plot_dec_reg, depth=IntSlider(min=1, max=5, value=1), criterion=['mse', 'mae'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на предсказание дерева. Что с ним не так и как с бы вы предложили с этим бороться?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша задача, выявить **линейную** зависимость между признаками в $X$ и значениями в $y$:\n",
    "$$\\hat{y} = X\\beta \\quad \\Leftrightarrow \\quad \\hat{y}^{(i)} = \\beta_0 + \\beta_1x^{(i)}_1 + \\dots$$\n",
    "То есть необходимо оценить коэффициенты $\\beta_i$.\n",
    "\n",
    "В случае линейной регрессии коэффициенты $\\beta_i$ рассчитываются так, чтобы минимизировать сумму квадратов ошибок по всем наблюдениям:\n",
    "$$ L(\\beta) = \\frac{1}{2n}(\\hat{y} - y)^{\\top}(\\hat{y} - y) = \\frac{1}{2n}(X\\beta - y)^{\\top}(X\\beta - y) \\rightarrow \\min$$ $$ \\Updownarrow $$  $$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 + \\dots - y^{(i)})^2  \\rightarrow \\min $$\n",
    "\n",
    "Несколько способов решения этой задачи:\n",
    "* Градиентный спуск \n",
    "* Normal Equations (Проекционные матрицы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если система $$ A x = b $$ не имеет решения, то решайте $$A^\\top A x = A^\\top b$$\n",
    "Отсюда получаем $$x = (A^\\top A)^{-1} A^\\top b$$\n",
    "\n",
    "Такое же выражение для коэффициентов можно получить через матричное дифференцирование функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задачка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите tutorial_dataset.csv или tutorial_dataset_2.csv в матрицу (таблицу) D. Присвойте y = D[:,0] а X = D[:, 1:].\n",
    "\n",
    "Оцените коэффициенты линейной регрессии $\\hat{y} = X\\hat{\\beta}$, где\n",
    "\n",
    "$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\n",
    "Остатки модели рассчитываются как\n",
    "$$ \\text{res} = y - \\hat{y} $$\n",
    "\n",
    "3. Постройте два графика: \n",
    "    1. Выберите какой-нибудь признак (кроме x0) и на одном графике изобразите зависимость y~x и линию регрессии\n",
    "    2. Постройте график  $\\hat{y}$~остатки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('tutorial_dataset_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(y_hat, res, marker='+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(\\beta_0, \\beta_1) = \\frac{1}{2n}\\sum_{i=1}^n(\\beta_0 + \\beta_1x_1^{(i)} - y^{(i)})^2$$ \n",
    "\n",
    "* Предположим мы выбрали какое-то начальное приближение $(\\hat{\\beta_0}, \\hat{\\beta_1})$\n",
    "* Его можно постараться улучшить - надо двигаться в сторону наискорейшего убывания функции (Антиградиента!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/dir-der.gif'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть уравнение регрессии имеет вид $\\hat{y} = \\beta_0 + \\beta_1x$\n",
    "\n",
    "Функция потерь $$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1- y^{(i)})^2 $$\n",
    "\n",
    "Посчитаем, чему равен градиент функции потерь $L(\\beta_0, \\beta_1):$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_0} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})$$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_1} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})x_1^{(i)}$$\n",
    "\n",
    "Иногда проще это записать в виде матриц:\n",
    "$$ \\frac{\\partial L}{\\partial \\beta} = X^\\top(X\\beta - y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод градиентного спуска заключается в итеративном и **одновременном(!!!)** обновлении значений $\\beta$ в направлении, противоположному градиенту:\n",
    "$$ \\beta := \\beta - \\alpha\\frac{\\partial L}{\\partial \\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом шаги алгоритма следующие:\n",
    "\n",
    "* Задаем случайное начальное значение для $\\beta$\n",
    "* Пока не будет достигнуто правило останова:\n",
    "    * Считаем ошибку и значение функции потерь\n",
    "    * Считаем градиент\n",
    "    * Обновляем коэффициенты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборку придется отнормировать для сходимости метода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = (X_train - X_train.mean())/X_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просто для удобства\n",
    "X_model = np.c_[np.ones(X_train.shape), X_train]\n",
    "X_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0 = np.linspace(11000 , 13000, 100)\n",
    "beta1 = np.linspace(-2450, -250, 100)\n",
    "\n",
    "B0, B1= np.meshgrid(beta0, beta1)\n",
    "\n",
    "B_all = np.c_[B0.reshape(-1,1), B1.reshape(-1,1)].T\n",
    "\n",
    "L = X_model.dot(B_all) - y_train.reshape(-1,1)\n",
    "L = L ** 2\n",
    "L = L.mean(axis=0)/2\n",
    "L = L.reshape(B0.shape)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.view_init(40, 25)\n",
    "ax.plot_surface(B0, B1, L, alpha=0.3,)\n",
    "ax.set_xlabel(r'$\\beta_0$')\n",
    "ax.set_ylabel(r'$\\beta_1$')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "contour = ax.contour(B0, B1, L)\n",
    "plt.clabel(contour, inline=1, fontsize=10)\n",
    "ax.set_xlabel(r'$\\beta_0$')\n",
    "ax.set_ylabel(r'$\\beta_1$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, iters, alpha):\n",
    "    \n",
    "    costs = []\n",
    "    betas = []\n",
    "    \n",
    "    n = y.shape[0] \n",
    "    Beta = np.random.rand(X.shape[1])\n",
    "    for i in xrange(iters):\n",
    "        y_hat = X.dot(Beta)\n",
    "        \n",
    "        # Ошибка и остатки\n",
    "        \n",
    "        # Градиент\n",
    "\n",
    "        # Обновление весов\n",
    "        Beta = Beta - (alpha/n)*grad\n",
    "        betas.append(Beta)\n",
    "                    \n",
    "    return Beta, costs, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Beta, costs, betas = gradient_descent(X_model, y_train, 100, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta0 = np.linspace(11000 , 13000, 100)\n",
    "beta1 = np.linspace(-2450, -250, 100)\n",
    "\n",
    "B0, B1= np.meshgrid(beta0, beta1)\n",
    "\n",
    "B_all = np.c_[B0.reshape(-1,1), B1.reshape(-1,1)].T\n",
    "\n",
    "L = X_model.dot(B_all) - y_train.reshape(-1,1)\n",
    "L = L ** 2\n",
    "L = L.mean(axis=0)/2\n",
    "L = L.reshape(B0.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "contour = ax.contour(B0, B1, L)\n",
    "plt.clabel(contour, inline=1, fontsize=10)\n",
    "ax.set_xlabel('beta_0')\n",
    "ax.set_ylabel('beta_1')\n",
    "\n",
    "betas = np.array(betas)\n",
    "ax.plot(betas[:,0], betas[:,1], marker='*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У градиентного спуска есть множество модификаций связанных. Некоторые улучшают процесс сходимости (градиент с импульсом), другие оптимизируют работу на больших выборках (стохастический градиентный спуск).\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\"><img src='http://sebastianruder.com/content/images/2016/09/contours_evaluation_optimizers.gif'></th>\n",
    "    <th class=\"tg-031e\"><img src='http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif'></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "[Обзор методов 1](http://sebastianruder.com/optimizing-gradient-descent/), [обзор методов 2](https://medium.com/towards-data-science/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f)\n",
    "\n",
    "Конечно же в sklearn [имплементирован](http://scikit-learn.org/stable/modules/sgd.html) стохастический градиентный спуск с большим выбором параметов.\n",
    "\n",
    "Стохастический градиентный достаточно гибок в использовании: \n",
    "* Его можно просто дообучать на новых данных (место того, чтобы строить новую модель с 0)\n",
    "* Можно оптимизировать более сложные функции потерь\n",
    "* Можно обучать на данных, которые не влезают в память\n",
    "\n",
    "По поводу последнего, конечно же лучше использовать специализированный софт, например [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format). Да, придется писать в командной строке и записывать данные в специальном формате, но поверьте, это того стоит [(пример)](https://habrahabr.ru/company/mlclass/blog/248779/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Природа зависимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далеко не всегда переменные зависят друг от друга именно в том виде, в котором они даны. Никто не запрещает зависимость вида\n",
    "$$\\log(y) = \\beta_0 + \\beta_1\\log(x_1)$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1\\frac{1}{x_1}$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1\\log(x_1)$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1 x_1^2 + \\beta_2 x_2^2 + \\beta_3 x_1x_2 $$\n",
    "и т.д.\n",
    "\n",
    "Не смотря на то, что могут возникать какие-то нелинейные функции - всё это сводится к **линейной** регрессии (например, о втором пункте, произведите замену $z_1 = \\frac{1}{x_1}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите данные `weights.csv` c информацией о весах мозга и тел различных биологических видов. Вес тела задан в килограммах, вес могза в граммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weights.csv', sep=';', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x = 'body_w', y='brain_w', kind='scatter')\n",
    "for k, v in df.iterrows():\n",
    "    plt.annotate(k, v[:2])\n",
    "# Должно получится что-то несуразное.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте возьмем логарифм от обеих переменных и снова нарисуем их на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_body_w'] = np.log(df.body_w)\n",
    "df['log_brain_w'] = np.log(df.brain_w)\n",
    "df.plot(x = 'log_body_w', y='log_brain_w', kind='scatter')\n",
    "for k, v in df.iterrows():\n",
    "    plt.annotate(k, v[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ладно, давайте дальше в sklearn =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.mileage.values.reshape(-1, 1)\n",
    "y_train = df_train.price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Model:\\nprice = %.2f + (%.2f)*mileage' % (model.intercept_, model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.plot(x='mileage', y='price', kind='scatter', s=120)\n",
    "\n",
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переобучение\\недообучение, мультиколлинеарность и регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из важнейших характеристик моделей, будь то линейная регрессия, наивные Байес и др. - их **обобщающая способность**.\n",
    "Наша задача не построить \"идеальную\" модель, на имеющихся у нас наблюдениях, которая идеально их будет предсказывать, но и применять эту модель для новых данных.\n",
    "\n",
    "Ниже приводятся примеры 3х моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=http://www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning_files/Image%20%5B8%5D.png>\n",
    "[Andrew's Ng Machine Learning Class - Stanford]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй момент, который важен для линейных моделей - **мультиколлинеарность**. Этот эффект возникает, когда пара предикторов  близка к взаимной линейной зависимости (коэффициент корреляции по модулю близок к 1). Из-за этого:\n",
    "\n",
    "* Матрица $X^{\\top} X$ становится плохо обусловленной и необратимой\n",
    "* Зависимость $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2$ перестаёт быть одназначной\n",
    "\n",
    "С этим эффектом можно бороться несколькими способами\n",
    "\n",
    "* Последовательно добавлять переменные в модель\n",
    "* Исключать коррелируемые предикторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обоих случаях может помочь **регуляризация** - добавление штрафного слагаемого за сложность модели в функцию потерь. В случае линейной регрессии было:\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "Стало (Ridge Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\left[ \\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 \\right] + \\lambda\\sum_{j=1}^{m}\\beta_j^2$$\n",
    "или (Lasso Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\left[ \\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 \\right] + \\lambda\\sum_{j=1}^{m}|\\beta_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/regul.jpg'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбросы в данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Квадратичная ошибка достаточно чувствительна к выбросам. Давайте вернемся к нашим данным про автомобили и добавим туда выбросы.\n",
    "\n",
    "Посмотрим, как поведет себя простая линейная регрессия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('http://bit.ly/1gIQs6C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.mileage.values.reshape(-1, 1)\n",
    "y_train = df_train.price.values\n",
    "\n",
    "n = y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем выброс(-ы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.r_[X_train, [[250000+np.random.rand()*10000]]]\n",
    "y_train = np.r_[y_train, 16000+np.random.randn()*1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Обучим 2 модели. Первая - на данных без выбросов. Вторая - на всех данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X_train[:n], y_train[:n])\n",
    "\n",
    "model_ouliers = LinearRegression(fit_intercept=True)\n",
    "model_ouliers.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = model.predict(x)\n",
    "y_hat_outliers = model_ouliers.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='red')\n",
    "ax.plot(x, y_hat_outliers, c='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANSAC регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея метода RANdom SAmple Consensus (RANSAC) заключается в многократном обучении модели на случайном наборе точек из исходных данных с последующим выбором лучшей модели.\n",
    "\n",
    "То есть:\n",
    "* Задаем функцию потерь\n",
    "* Задаем порог $\\theta$ для остатков при котором наблюдения начинают относится к выбросам\n",
    "* Задаем правило останова\n",
    "\n",
    "Шаги алгоритма следующие\n",
    "1. Взять случайные K точек и обучить на них модель M\n",
    "2. Сравнить ошибки на остальных точких с порогом $\\theta$ и отнести к выбросам или внутренним точкам\n",
    "3. Обучить модель на всех внутренних точках, оценить качество на внутренних точках\n",
    "4. Повторить 1-3 пока не наступит правило останова. \n",
    "5. Вывод: модель с лучшим качеством"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ransac = RANSACRegressor(LinearRegression())\n",
    "model_ransac.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = model_ransac.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='red')\n",
    "ax.plot(x, y_hat_outliers, c='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея робастных методов заключается во взвешивании остатков модели таким образом, чтобы большие значения вносили меньший вклад в оценку параметров.\n",
    "\n",
    "Таким образом, вместо минимизации квадрата остатков $$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "Будут минимизироваться взвешенные остатки $$ L_w(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}\\rho_i \\cdot (\\hat{y}^{(i)} - y^{(i)})^2,$$\n",
    "где $\\rho_i$ - некоторый вес\n",
    "\n",
    "Для того, чтобы попробовать эти методы нужно будет устновить пакет `statsmodels` через `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 4.685\n",
    "support = np.linspace(-3*c, 3*c, 1000)\n",
    "tukey = sm.robust.norms.TukeyBiweight(c=c)\n",
    "plt.plot(support, tukey(support))\n",
    "plt.ylim(.1, -4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полный список взвешивающих функций в модуле `statsmodels` можно найти [тут](http://statsmodels.sourceforge.net/stable/examples/notebook/generated/robust_models_1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_robust = sm.RLM(y_train, sm.add_constant(X_train), M=sm.robust.norms.TukeyBiweight())\n",
    "model_robust = model_robust.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_robust.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = model_robust.predict(sm.add_constant(x))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='red')\n",
    "ax.plot(x, y_hat_outliers, c='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим остальные признаки в простую модель модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "347px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "none",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "toc_position": {
   "height": "40px",
   "left": "816px",
   "right": "38.6667px",
   "top": "0px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
